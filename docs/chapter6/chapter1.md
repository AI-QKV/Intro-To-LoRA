# LoRA（六）QLoRA量化的支持



##### 再次感谢DataWhale，让AI进入更多人的视野，也感谢社区里面默默付出的人，让我学到了很多人工智能的知识，让我快速的成长。
##### 作者：小韩（前快手高级工程师，前58高级工程师，现Datawhale社区成员）




 # 平台支持和配置信息
 
CUDA，这个是英伟达推出的并行计算平台，本质就是可以让开发者最大程度的利用GPU的数千个核心并行执行，从而显著提升应用程序的运行速度。
 
我们之前在讲QLoRA的时候，用的是BitsAndBytesConfig这个配置文件来进行的，这里我们来说明一下，BitsAndBytesConfig目前(2025年11月)支持最好的平台是英伟达的CUDA平台，BitsAndBytesConfig在这个平台也是最稳定，功能最全。所以市面上默认一般QLoRA都是跑在CUDA平台
     
ROCm，这个是AMD的开源GPU计算生态系统‌，主要应用于人工智能、机器学习、科学计算及数据中心加速等领域。目前在这个平台配置BitsAndBytesConfig的时候，通常要用第三方的版本来运行QLoRA，(bitsandbytes-rocm），现阶段使用过程和主流平台有所差异，但随着时间发展行业的迭代并且加上AMD持续更新，相信也会达到使用方便的程度。

Apple Silicon (M1/M2/M3/等等)，这个是苹果公司推出的低功耗芯片，目前在移动平台性能优异，在台式机上表现也非常出色。
由于BitsAndBytesConfig目前跑不了这种芯片，但是Apple也推出了替代方案，可以使用MLX来进行，比如使用mlx-lm库（同样支持4-bit量化的LoRA微调）。

AutoGPTQ，这个不是平台，这个是基于GPTQ算法的开源模型量化工具包，将大语言模型（如BERT、OPT、LLaMA等）转换为低精度版本（如4位整数），以减少模型体积并提升推理速度。QLoRA默认使用NF4格式（动态解压），AutoGPTQ 使用 INT4 格式。对于非 CUDA 显卡（如 AMD），AutoGPTQ 的兼容性通常比 bitsandbytes好。
     
Unsloth，微调工具，这个是目前最强的微调工具，它的原理是，重新写了QLoRA的反向传播，导致性能有很大的提升，在速度方面比之前标准的QLoRA提升了2倍，但是目前只能在NVIDIA平台，对AMD的支持还在开发中。
     
 


# 卡的选择
	
	之前的文章中已经讲解了BaseModel模型配置信息，浮点数，训练配置等等，这里在简单讲一下模型的显存占用，当选择一个模型后，比如32B的大模型，如果使用的是FP16，我们可以进行一下计算，32B * 2bytes，那么大约就是64G的显卡。
	常见的卡的配置有：
		NVIDIA (H100–80GB)
		NVIDIA (A100–80GB)
		NVIDIA (L40–48GB)
		NVIDIA（V100–32GB)
		NVIDIA (4090–24GB)
		AMD(MI250X‌—128GB)
		AMD （MI100‌—32GB）
		AMD （7900XTX—24GB）
		
	刚刚我们对常见的卡的型号已经有所了解，现在我们还需要明白卡的类型，卡分为消费级和专业级，上面的RTX-4090就是消费级的卡，当然优点是性价比高，缺点是不支持NVLink（3090支持，4090不支持），而专业级的卡肯定是要比消费级别的卡更好，特别在图像方面和推理方面，也支持NVLink，CUDA Graph, DDP（多卡并行）等特点。

	一般情况下，只要是搞AI的公司手里至少有30-50张卡，毕竟AI是核心业务（训练、调试、并发、推理，还有数据安全性等等），在目前的AI浪潮中，总不能掉队吧，即便是小公司，有个5张到10张卡是正常现象。但是，即便这样，很多时候业务量增加的情况，我们还需要借助云平台来进行。
	
	

# 显存占用
	上面举例了32B模型如果使用FP16占用的大小，那么如果模型使用的是FP32(float32)，那么就是4个字节，就是32B * 4bytes,需要128G的显卡，常见精度都有：
	FP32，占用4个字节
	FP16，占用2个字节
	BF16，占用2个字节
	INT8，占用1个字节
	INT4，占用0.5字节
	这块我们在量化当中有讲过，大家可以回顾上一篇文章来加深印象。
	注意： 以上这只是“模型权重”占用的静态显存。要让模型跑起来（无论是推理还是训练），还需要额外的空间（KV Cache、缓冲区、激活值等）。
	
	在推理阶段，显存占用主要由三部分组成：模型权重 + KV Cache (上下文) + 缓冲区
	模型权重：就是我们静态计算的值。
	KVCache：这个就是上下文，内容越长，占用越大。
	缓冲区：模型运行及系统开销等其他占用，建议2G以上。
	根据这个上面的内容，我们就可以预估训练，推理，要使用的显存大小。

	如果我们使用的是QLoRA来进行的话，那么使用4-bit精度，并且控制好上下文长度，那么以32B模型举例（注意：这里只是举例）：
	32B +QLoRA (4-bit）+ Gradient Checkpointing (梯度检查点) = 24 GB (3090显卡，很极限即使运行也非常困难且不稳定的)
	通过上面的显存分析，我们可以了解，QLoRA在性价比方面具有强大的优势。
	

	

# 常见问题

    Q：为什么LoRA和QLoRA有了Rank还需要缩放因子alpha
    A：
    Rank的作用是控制A(input, rank)和(rank, output)的参数训练量，而缩放因子alpha（通常a=rank直或者是a=2rank）是控制rank的参数权重。
    
    Q：为什么既然有了rank和alpha还需要设置TrainingArguments的learning_rate学习率
    A：
    原始模型的权重，就像一片宽5米和长5米的土地，rank是挖掘机能对这个土地挖多深，而alpha是挖掘机能装多少，学习率是挖掘机进行挖掘的速度。如果学习率太大，表示挖的太快，容易超过土地边界，如果学习率太小，挖掘太慢，导致规定时间内完不成挖掘任务。
    
    Q：为什么LoRA和QLoRA的学习率是3e-4，而不是0.01
    A：
    由于是微调，在微调领域中，目前被广泛应用的微调范围中是包含3e-4的，所以这个也是一个被验证过的值，可以保持训练的稳定性。
    
    Q：LoRA的原理是什么
    A：
    LoRA公式已经表达的很清楚了，公式中W冻结，AB(rank)进行训练。训练后合并，这样合并后的’W，就有了AB(rank)的参数。
    
    
    Q：QLoRA显存不足怎么办
    A：
    QLoRA确实节省了很多显存，但是训练中我们还需要注意Batch Size和Sequence Length，可以调低批量大小和序列长度，如果还是有OOM的问题，可以开启梯度检查点，如果还是有问题，只能升级显卡或者用其他的模型了。
    
    Q：没有NVIDIA卡，怎么使用QLoRA
    A：
    QLoRA 默认基于 CUDA 和 BitsAndBytes，因此在 NVIDIA 平台上最稳定。 AMD卡可以尝试使用 bitsandbytes-rocm 社区版本，或使用 AutoGPTQ（基于 INT4）等工具包进行量化微调，因为 AutoGPTQ 在非 CUDA 平台上的兼容性通常更好。 Apple Silicon芯片BitsAndBytes 不支持，但可以转向 MLX 框架，它提供了原生支持 Apple 芯片的 4-bit LoRA 微调方案。
    
    
    Q： QLoRA 的性能真的能媲美全量微调吗？
    A：
    在大多数任务上，QLoRA 能够以极小的参数量实现接近全量微调的性能。但对于需要模型进行大幅度知识修正或多领域知识融合的复杂任务，全量 FP16 微调由于能够调整所有 Base Model 权重，性能上限依然略高。在实践中，QLoRA是成本效益的最佳选择。





最后结语

大模型虽然在蓬勃发展，但是发展到某个阶段后，如果不掌握微调，很快我们就会被各种阉割的数据，失真的数据被淹没在“数字饲料”或者“信息饲料”当中，成为被驯化的一环，掌握微调在未来的某个节点将会是一项必备的技能。总而言之，经过前面文章对LoRA的了解，到现在对 QLoRA 的代码示例和量化的原理了解，我相信各位读者已经慢慢的在微调的道路上有了自己的见解，并且也越走越远，最后的结语，我要说的是，实践出真知，在训练的过程中，掌握原理是前提，敢于挑战困难是勇气，持续迭代才是通向高性能企业级 LLM 的最终路径。
