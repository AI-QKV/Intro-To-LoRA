# LoRA(八)模型评估
###### 作者：小韩（前快手高级工程师，前58高级工程师，现Datawhale社区成员）


# 模型评估

目前大模型评估有两个基础指标，第一个是准确率，第二个是召回率。这两个指标并非只应用于大模型，指标的起始来源于信息检索，最早是在20世纪50-60年代的信息检索领域‌被提出，指标用于评估搜索引擎返回结果和质量。如果把指标放在医疗领域、安全领域的话，这两个指标还衍生出了F1值等平衡指标。	还有一个地方容易混淆的是，在信息检索中，召回率有时也被称为“查全率”，而准确率有时被称为“查准率”，但这两个术语在有时会混用，下面是指标的解释。

准确率：核心就是“预测”的准确性。通过预测，查看正确的内容是多少，正确的比例是多少。

召回率：核心就是“找回”的完整性。在所有实际的正类别中，找回了多少，发现了多少。

F1值：一个综合指标，尤其‌在数据分布不平衡时‌，这个值能更全面地反映整体情况。

上面的内容就是指标概念，那么在大模型领域中，我们如何对一个模型进行评估，我自己的理解可以通过几个步骤来进行，这些可以编辑成一个评估脚本，让脚本包含了如下内容：

1，Test Set测试集数据：注意这部分数据绝对不能出现在训练集中

2，批量推理：使用合并后的模型对测试集进行预测

3，计算指标：通过准确率和召回率来评估

4，幻觉情况：通过前三步内容后，测试大模型输出的情况

5，其他方式评估等等



# Loss是否最优
	
我在训练大模型的时候，正常情况是一定要看Loss的，我曾以为Loss也是越低越好，其实现在想想这是一个误区。Loss只能表示模型学了知识，但是有没有学会知识，有没有真的学到知识，是另外一个情况。而评估Loss是否最优还需要结合3种情况来分析：

1，欠拟合：Loss高，或者数据质量差等问题导致的。

2，过拟合：Loss低，重复内容太多或回答的太过标准。

3，最佳：Loss 下降并稳定且验证集（Validation Set）的准确率同步上升。

在实际训练过程中，还需要考虑BaseModel，参数量，以及训练的轮次，其他配置信息等，所以上面的内容也只是我个人的理解。



	
# 意图的层级
之前的文章三写了一点意图的内容，这里还需要再进行一个补充。我们先要确定模型要达到一个什么样的效果，其实是可以分成几个步骤的：

  1，基础内容：通过基础内容保证模型的输出是符合规范的。
  
  2，进阶内容：通过CoT或者上下文的判断了解意图，从而推理思考并输出。
  
  3，高级内容：通过数据的边界和训练中的情况来精准区分，从而达到防止幻觉的输出。
  
  4，其他方式等等。
	
  	如果还是达不到理想的情况，那么可以参考我之前的几种方式
  	1，从准备的数据中获取，也就是我们目前的情况check_logistics
  	2，对数据进行进行处理，添加更多关联，比如：思维链CoT，标签Tagging等方式
  	3，Rank的参数设置，Alpha的参数设置等，以及训练的轮次
  	4，是否需要调用对应服务，比如使用MCP等
  	5，根据业务需要是否使用Rag或者图数据库等
  	6，根据我在github.com/AI-QKV（提过的模块优化公式来进行）
    

如果还是达不到，那么可以自己写一个小模型来进行，从而到达专业级的内容输出和回答。
由于我们这里讲的是LoRA和QLoRA，其他的内容我们只是简单了解就行。




# 推理原理
这一段内容，我们在说一下原理，在transforms中，多头注意力通过N次计算之后，从词表当中选择了几个词，然后通过softmax来确定最后要输出的是哪个词。
所以不管如何微调模型，无论采用LoRA、QLoRA还是全参数微调，最终都遵循这个基础推理原理
模型训练的目标就是优化参数，使得在给定上下文时，正确词能获得最高的softmax概率。


# 最后的结语：
这篇内容就是我对模型训练的的理解，如果有更有效更严谨的方式，欢迎留言讨论，从而推动大模型领域在国内加速发展，普及更多的人。
