# LoRA深入浅出(四)量化调优QLoRA


###### 作者：小韩  前快手高级工程师，前58高级工程师，现Datawhale社区成员

###### 原创不易，转载请保留署名

# 浮点数

LoRA相关的先到此为止，我们开始QLoRA，在开始QLoRA之前，我们先快速了解一下浮点数在计算机中是如何表示的，
在IEEE中，定义了浮点数的规范，比如‌单精度（32位）和‌双精度（64位）等格式，在二进制中通过3部分表示这里我们是快速了解，我们举例一个简单的例子来说明，用一个32位的浮点举例，
如：-101.0000001

1，符号位：占用1位，0表示正数，1表示负数
2，阶码位：占用8位，存储实际指数
3，尾数位：占用23位，存储小数点后面的值

1位就是1个bit

bit（比特）：计算机存储的最小单位，表示二进制中的0或1
Byte（字节）：由8个bit组成，是存储和传输的基本单位，二进制中用的2的8次方表示以上就是浮点数的内容，我们开始QLoRA


# QLoRA的公式

QLoRA公式Output = W_quantized + (AB)x

下面是说明
Output:  表示输出的向量
W_quantized:  表示经过量化后的权重矩阵
AB:  表示LoRA中的矩阵
x:  表示输入的向量

QLoRA技术栈公式QLoRA = 4bit量化 + LoRA + 双重量化 + 分页器优化

下面是说明
4bit量化：节省内存，将于训练模型的权重量化为4位的精度
LoRA：通过低秩矩阵来进行参数高效微调
双重量化：对量化参数进行再次量化，进一步压缩存储，这里主要是对缩放因子处理，此参数用可以来控制LoRA权重
分页器优化：管理显存，通过分页优化器状态在训练过程中减少资源占用，从而防止内存溢出



# QLoRA代码示例








